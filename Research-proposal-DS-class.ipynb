{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RoiLe/AI-picture-recognition/blob/Roi-dev/demo_AI_img_rec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSrPCU_FtJ6H"
      },
      "source": [
        "# Data Science final project - Fake & Real images recognition\n",
        "#### by Lior Guetta & Roi Levi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WH7hkKCatJ6H"
      },
      "source": [
        "<h1>Problem description</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8VcZ3dTxtJ6I"
      },
      "source": [
        "\n",
        "\n",
        "In today's digital age, the rise of artificial intelligence has brought us to a time when it's tricky to tell if a picture is real or made by a computer, AI-powered tools now have the capability to produce images that can be virtually indistinguishable from those captured by a camera. This project is dedicated to the critical task of discerning between genuine and AI-generated images.\n",
        "\n",
        "The proliferation of AI-generated images presents a unique set of challenges. With the potential for images to be manipulated or fabricated, it becomes imperative to develop robust methods for distinguishing fact from fiction. This project aims to address this issue by exploring techniques that can reliably determine whether an image was created by an AI tool or is a product of the physical world.\n",
        "\n",
        "Throughout this project, we will go deeply into various aspects of image analysis, datasets containing both authentic and AI-generated images, and advanced computational methods. By doing so, we aim to provide a valuable contribution to the ongoing discourse on the credibility of digital visuals.\n",
        "\n",
        "\n",
        "As we embark on this journey to distinguish real from AI-generated images, we invite you to join us in unraveling the complexities of this modern-day challenge. Together, we aspire to contribute to a future where digital images can be scrutinized and trusted with confidence.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GRlji6OgB7N"
      },
      "source": [
        "#### requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ezz5dqZBgFyZ"
      },
      "outputs": [],
      "source": [
        "%pip install opendatasets\n",
        "%pip install pandas\n",
        "%pip install seaborn\n",
        "%pip install matplotlib.pyplot\n",
        "%pip install tensorflow\n",
        "%pip install numpy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HlpiMyJGjM7s"
      },
      "source": [
        "#### imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6nhQ4DOZiKMa"
      },
      "outputs": [],
      "source": [
        "import opendatasets as od\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import shutil\n",
        "import random\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import seaborn as sns\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9XAXULFXjT9K"
      },
      "source": [
        "<h2>Data gathering</h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "The CIFAR-10 dataset (Canadian Institute For Advanced Research) is a collection of images that are commonly used to train machine learning and computer vision algorithms. It is one of the most widely used datasets for machine learning research. The CIFAR-10 dataset contains 60,000 32x32 color images in 10 different classes. The 10 different classes represent airplanes, cars, birds, cats, deer, dogs, frogs, horses, ships, and trucks. There are 6,000 images of each class. \n",
        "\n",
        "For the FAKE images, the data collector (thanks for Kagg) generated the equivalent of CIFAR-10 with Stable Diffusion version 1.4."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YPekt9XminLc",
        "outputId": "76d5fb11-48ce-4544-9fc0-ae4a3d825f60"
      },
      "outputs": [],
      "source": [
        "od.download(\"https://www.kaggle.com/datasets/birdy654/cifake-real-and-ai-generated-synthetic-images\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bgg5fk_-QWAc"
      },
      "source": [
        "First of all we will display some data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vr6BbVcaQWAc"
      },
      "outputs": [],
      "source": [
        "# reading the XLSX file\n",
        "fake_folder =('cifake-real-and-ai-generated-synthetic-images\\\\train\\FAKE')\n",
        "\n",
        "images_path = []\n",
        "for filename in os.listdir(fake_folder):\n",
        "  img = os.path.join(fake_folder,filename)\n",
        "  if img is not None:\n",
        "      images_path.append(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BGEd2TOJAb8H",
        "outputId": "db989137-6081-4c1c-bfa4-5c2004c3f0aa"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create a figure with 5 subplots side by side\n",
        "fig, axes = plt.subplots(1, 5, figsize=(8, 4))\n",
        "fig.suptitle(\"Random fake images\", fontsize=16)\n",
        "\n",
        "# Loop to display 5 random FAKE images\n",
        "for i in range(5):\n",
        "    j = random.randint(0, 39999)\n",
        "    image = cv2.imread(images_path[j])\n",
        "\n",
        "    # Check if the image was loaded successfully\n",
        "    if image is not None:\n",
        "        # Displaying the image in the corresponding subplot\n",
        "        axes[i].imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "    else:\n",
        "        print(f\"Error loading image {images_path[j]}\")\n",
        "\n",
        "# Remove axis labels and ticks\n",
        "for ax in axes:\n",
        "    ax.axis('off')\n",
        "\n",
        "# Show the plot\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dwaN8zotJ6N"
      },
      "source": [
        "<h1>EDA</h1>\n",
        "<h3>Exploratory Data Analysis</h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJhXVHQgAb7_"
      },
      "source": [
        "In our quest to distinguish authentic images from AI-generated ones, Exploratory Data Analysis (EDA) plays a pivotal role.\n",
        "\n",
        "EDA is our compass through the sea of data, guiding us to uncover critical insights about image characteristics.\n",
        "\n",
        "Our primary focus during this analysis will revolve around four key aspects: **the number of colors, sharpness, brightness, and contrast.**\n",
        "\n",
        "Through this focused EDA, we aim to uncover unique insights into these specific image attributes, enabling us to make informed decisions regarding data preprocessing, feature selection, and model development.\n",
        "\n",
        "Our journey through these aspects of image analysis will be instrumental in building a robust system for differentiating between real and AI-generated images.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OUimL3_bAb7_"
      },
      "outputs": [],
      "source": [
        "# path to the directories:\n",
        "base_path = './cifake-real-and-ai-generated-synthetic-images'\n",
        "TEST, TRAIN = 'test', 'train'\n",
        "REAL, FAKE = 'REAL', 'FAKE'\n",
        "test_fake = os.listdir('./cifake-real-and-ai-generated-synthetic-images/test/FAKE')\n",
        "test_real = os.listdir('./cifake-real-and-ai-generated-synthetic-images/test/REAL')\n",
        "train_fake = os.listdir('./cifake-real-and-ai-generated-synthetic-images/train/FAKE')\n",
        "train_real = os.listdir('./cifake-real-and-ai-generated-synthetic-images/train/REAL')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hM9QQeBzAb8A"
      },
      "outputs": [],
      "source": [
        "# some variables..\n",
        "arr_fake = []\n",
        "arr_real = []\n",
        "arr_fake_num_colors = []\n",
        "arr_real_num_colors = []\n",
        "arr_fake_sharpness = []\n",
        "arr_real_sharpness = []\n",
        "arr_fake_brightness = []\n",
        "arr_real_brightness = []\n",
        "arr_fake_contrast = []\n",
        "arr_real_contrast = []\n",
        "max_colors = 10000\n",
        "total_colors_fake = 0\n",
        "total_colors_real = 0\n",
        "total_sharpness_fake = 0\n",
        "total_sharpness_real = 0\n",
        "total_brightness_fake = 0\n",
        "total_brightness_real = 0\n",
        "total_contrast_fake = 0\n",
        "total_contrast_real = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FfFydIbPAb8A"
      },
      "source": [
        "We will go through all the images from each ds and examine:\n",
        "1) The number of colors in the picture.\n",
        "2) the sharpness of the image.\n",
        "3) The contrast of the colors in the picture.\n",
        "4) The clarity of the image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vR2WIsz5Ab8A"
      },
      "source": [
        "A. Fake images loop:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XYBNhdiOAb8A"
      },
      "outputs": [],
      "source": [
        "for picture in train_fake :\n",
        "    # read & open image\n",
        "    image = cv2.imread(os.path.join(base_path, TRAIN, FAKE, picture))\n",
        "    img = Image.open(os.path.join(base_path, TRAIN, FAKE, picture))\n",
        "\n",
        "    # convert to gray scale image.\n",
        "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    # 1. number of colors\n",
        "    total_colors_fake = total_colors_fake +(len(img.getcolors(max_colors)))\n",
        "    # 2. sharpeness\n",
        "    sharpness = cv2.Laplacian(gray, cv2.CV_64F).var()\n",
        "    total_sharpness_fake = total_sharpness_fake + sharpness\n",
        "    arr_fake_sharpness.append(sharpness)\n",
        "    # 3. brightness\n",
        "    brightness = int(gray.mean())\n",
        "    # 4. contarst\n",
        "    contrast = np.std(gray)\n",
        "    total_contrast_fake = total_contrast_fake +contrast\n",
        "    arr_fake_contrast.append(contrast)\n",
        "    total_brightness_fake = total_brightness_fake+brightness\n",
        "    arr_fake_brightness.append(brightness)\n",
        "    arr_fake_num_colors.append(len(img.getcolors(max_colors)))\n",
        "    arr_fake.append(image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRZ8e27KAb8A"
      },
      "source": [
        "B. Real images loop:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5c5BhLo9Ab8B"
      },
      "outputs": [],
      "source": [
        "for picture in train_real :\n",
        "    # read & open image\n",
        "    image = cv2.imread(os.path.join(base_path, TRAIN, REAL, picture))\n",
        "    img = Image.open(os.path.join(base_path, TRAIN, REAL, picture))\n",
        "\n",
        "     # convert to gray scale image.\n",
        "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    # 1. number of colors\n",
        "    total_colors_real = total_colors_real +(len(img.getcolors(max_colors)))\n",
        "    # 2. sharpeness\n",
        "    sharpness = cv2.Laplacian(gray, cv2.CV_64F).var()\n",
        "    total_sharpness_real = total_sharpness_real + sharpness\n",
        "    arr_real_sharpness.append(sharpness)\n",
        "    # 3. brightness\n",
        "    brightness = int(gray.mean())\n",
        "    # 3. contarst\n",
        "    contrast = np.std(gray)\n",
        "    total_contrast_real = total_contrast_real + contrast\n",
        "    arr_real_contrast.append(contrast)\n",
        "    total_brightness_real = total_brightness_real+brightness\n",
        "    arr_real_brightness.append(brightness)\n",
        "    arr_real_num_colors.append(len(img.getcolors(max_colors)))\n",
        "    arr_real.append(image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "luZJCDU6Ab8B",
        "outputId": "e668693d-2d7a-4d17-acfc-158f5676d1c8"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(8, 3))  # 1 row, 2 columns\n",
        "# Plot histogram for dataset 1 in the first subplot\n",
        "axes[0].hist(arr_fake_num_colors, bins=100, alpha=0.5, color='blue', label='fake')\n",
        "axes[0].set_xlabel('Number of colors')\n",
        "axes[0].set_ylabel('Count of images')\n",
        "axes[0].set_title('fake')\n",
        "axes[0].legend()\n",
        "\n",
        "# Plot histogram for dataset 2 in the second subplot\n",
        "axes[1].hist(arr_real_num_colors, bins=100, alpha=0.5, color='red', label='real')\n",
        "axes[1].set_xlabel('Number of colors')\n",
        "axes[1].set_ylabel('Count of images')\n",
        "axes[1].set_title('real')\n",
        "axes[1].legend()\n",
        "\n",
        "# Adjust layout and show the plot\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "print('Average number of colors for fake images: ',round((total_colors_fake/len(train_fake)),3))\n",
        "print('Average number of colors for real images: ',round((total_colors_real/len(train_real)),3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYrd13s9Ab8B"
      },
      "source": [
        "<b>Number of colors</b>\n",
        "\n",
        "We can derive from these graphical representations that fake images exhibit a reduced color spectrum. Notably, we discern three prominent peaks in the values at 200, 350, and 1000 within the fake images, whereas in the case of real images, these peaks are predominantly confined to the 1000 values.\n",
        "\n",
        "This insight holds the potential to assist us in future endeavors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "80jWcy8cAb8B",
        "outputId": "775df5ff-0dd8-4de1-bfe8-d01b01d217ce"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(8, 3))\n",
        "# Plot histogram for dataset 1 in the first subplot\n",
        "axes[0].hist(arr_fake_sharpness, bins=100, alpha=0.5, color='blue', label='fake')\n",
        "axes[0].set_xlabel('Sharpness value')\n",
        "axes[0].set_ylabel('Count of images')\n",
        "axes[0].set_title('fake')\n",
        "axes[0].legend()\n",
        "\n",
        "# Plot histogram for dataset 2 in the second subplot\n",
        "axes[1].hist(arr_real_sharpness, bins=100, alpha=0.5, color='red', label='real')\n",
        "\n",
        "axes[1].set_xlabel('Sharpness value')\n",
        "axes[1].set_ylabel('Count of images')\n",
        "axes[1].set_title('real')\n",
        "axes[1].legend()\n",
        "\n",
        "# Adjust layout and show the plot\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print('Average sharpness for fake images: ',round((total_sharpness_fake/len(train_fake)),3))\n",
        "print('Average sharpness for real images: ',round((total_sharpness_real/len(train_real)),3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FQYaYmxAb8C"
      },
      "source": [
        "<b>Sharpness</b>\n",
        "\n",
        "An intriguing deduction arising from our graphical representations is the unexpected observation that, on average, fake images manifest a significantly higher degree of sharpness, surpassing real images by a remarkable 50%. This rather surprising revelation bears the potential to assume a critical role in our differentiation efforts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1wt_uAxJAb8C",
        "outputId": "4b8e93a0-795e-425d-995f-37a7176514b1"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(8, 3))  # 1 row, 2 columns\n",
        "# Plot histogram for dataset 1 in the first subplot\n",
        "axes[0].hist(arr_fake_brightness, bins=100, alpha=0.5, color='blue', label='fake')\n",
        "axes[0].set_xlabel('Brightness of image')\n",
        "axes[0].set_ylabel('Count of images')\n",
        "axes[0].set_title('fake')\n",
        "axes[0].legend()\n",
        "\n",
        "# Plot histogram for dataset 2 in the second subplot\n",
        "axes[1].hist(arr_real_brightness, bins=100, alpha=0.5, color='red', label='real')\n",
        "axes[1].set_xlabel('Brightness of image')\n",
        "axes[1].set_ylabel('Count of images')\n",
        "axes[1].set_title('real')\n",
        "axes[1].legend()\n",
        "\n",
        "# Adjust layout and show the plot\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print('Average brightness for fake images: ',round((total_brightness_fake/len(train_fake)),3))\n",
        "print('Average brightness for real images: ',round((total_brightness_real/len(train_real)),3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YqZ0pR-VAb8C"
      },
      "source": [
        "<b>Brightness</b>\n",
        "\n",
        "These graphical representations exhibit striking similarities, albeit with subtle differentiations. Notably, real images display a marginally higher level of brightness compared to their AI-generated counterparts. While this discrepancy may initially appear inconsequential, it may potentially emerge as a pertinent factor contributing to the efficacy of the forthcoming Convolutional Neural Network (CNN) we intend to construct."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-upSBKKQAb8C",
        "outputId": "1bb57282-ae90-4348-aa45-f1d7848404e8"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(8, 3))  # 1 row, 2 columns\n",
        "# Plot histogram for dataset 1 in the first subplot\n",
        "axes[0].hist(arr_fake_contrast, bins=100, alpha=0.5, color='blue', label='fake')\n",
        "axes[0].set_xlabel('contrast of image')\n",
        "axes[0].set_ylabel('Count of images')\n",
        "axes[0].set_title('fake')\n",
        "axes[0].legend()\n",
        "\n",
        "# Plot histogram for dataset 2 in the second subplot\n",
        "axes[1].hist(arr_real_contrast, bins=100, alpha=0.5, color='red', label='real')\n",
        "axes[1].set_xlabel('contrast of image')\n",
        "axes[1].set_ylabel('Count of images')\n",
        "axes[1].set_title('real')\n",
        "axes[1].legend()\n",
        "\n",
        "# Adjust layout and show the plot\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print('Average contrast for fake images: ',round((total_contrast_fake/len(train_fake)),3))\n",
        "print('Average contrast for real images: ',round((total_contrast_real/len(train_real)),3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTa-4nXytT32"
      },
      "source": [
        "<b>Contrast</b>\n",
        "\n",
        "It is evident that these datasets exhibit a near-identical profile, distinguished by a subtle variation: fake images demonstrate a marginally higher average, albeit a mere 2% difference, which is essentially negligible. This parallels the negligible variation we've observed in brightness values. Our interest now lies in discerning whether these minute distinctions may hold relevance for potential enhancement of our Convolutional Neural Network (CNN) in subsequent phases of our analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_sjcoThftJ6N"
      },
      "source": [
        "<b>Conclusions</b>\n",
        "\n",
        "In our pursuit of distinguishing genuine from AI-generated images, our Exploratory Data Analysis (EDA) has uncovered vital insights. We've observed substantial disparities in the number of colors and sharpness between real and fake images, underscoring their potential as discriminators in our image authenticity verification system. These findings provide a solid foundation for further analysis and model development.\n",
        "\n",
        "On the other hand, our investigation into brightness and contrast revealed negligible variations between the two image categories. While it's uncertain how significant these statistics will be, they present an interesting avenue for exploration. The stability and consistency observed in brightness and contrast statistics might, albeit uncertainly, contribute to the effectiveness of our Convolutional Neural Network (CNN)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HhdTnRoitJ6N"
      },
      "source": [
        "<h1>Basic Algoritms</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CPa6H66iQWAg"
      },
      "source": [
        "In this chapter we will see a number of algorithms that will guide us to build the ideal model, we will see how HOGs analyzes the gradients of the images, we will use GANs and even try to convert the images to their frequency domain and look for certain relationships there. Hopefully at the end of the process we can build a model with ideal prediction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QwXFbOxftJ6O"
      },
      "source": [
        "<h2>HOGs</h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6g2thZA7Nb6C"
      },
      "source": [
        "Histogram of Oriented Gradients (HOG) is a potent image analysis technique that excels at capturing texture and shape details by analyzing local gradients within images. This method provides valuable insights into these structural features, aiding in the differentiation between authentic and AI-generated images. HOG's resilience to lighting and color variations makes it a key tool in enhancing image authenticity verification systems."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PLxwEI6mAb8D"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from skimage.feature import hog\n",
        "from skimage import exposure\n",
        "from skimage.color import rgb2gray\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stPOe0aPQWAg"
      },
      "source": [
        "<h4>HOGs preprocess</h4>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FBXwdG54Ab8D"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Function to load and process image data from a directory\n",
        "def load_images_from_directory(directory):\n",
        "    data = []\n",
        "    for filename in os.listdir(directory):\n",
        "        if filename.endswith(\".jpg\"):  # Assuming your images are JPEG files\n",
        "            img_path = os.path.join(directory, filename)\n",
        "            img = Image.open(img_path)\n",
        "            img = np.array(img)  # Convert the image to a NumPy array\n",
        "            data.append(img)\n",
        "    return np.array(data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C2ShA4nmAb8E"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Function to compute HOG features for a list of images\n",
        "def compute_hog_features(images):\n",
        "    flag = 1\n",
        "    hog_features = []\n",
        "    for img in images:\n",
        "        gray_img = rgb2gray(img)\n",
        "        # Compute HOG features\n",
        "        fd, hog_image = hog(gray_img, pixels_per_cell=(8, 8), cells_per_block=(2, 2), visualize=True)\n",
        "        hog_features.append(fd)\n",
        "        if(flag == 1):\n",
        "            # Plot gradient histogram\n",
        "            plt.figure(figsize=(5, 5))\n",
        "            plt.subplot(121)\n",
        "            plt.imshow(gray_img, cmap=plt.cm.gray)\n",
        "            plt.title('Input Image')\n",
        "\n",
        "            # Rescale histogram for better visualization\n",
        "            hog_image_rescaled = exposure.rescale_intensity(hog_image, in_range=(0, 10))\n",
        "\n",
        "            # Plot HOG features\n",
        "            plt.subplot(122)\n",
        "            plt.imshow(hog_image_rescaled, cmap=plt.cm.gray)\n",
        "            plt.title('HOG Features')\n",
        "            plt.show()\n",
        "            flag = 0\n",
        "    return hog_features\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qLa1yQuPAb8E"
      },
      "outputs": [],
      "source": [
        "# Directory paths\n",
        "train_fake_dir = \"cifake-real-and-ai-generated-synthetic-images\\\\train\\FAKE\"\n",
        "train_real_dir = \"cifake-real-and-ai-generated-synthetic-images\\\\train\\REAL\"\n",
        "test_fake_dir = \"cifake-real-and-ai-generated-synthetic-images\\\\test\\FAKE\"\n",
        "test_real_dir = \"cifake-real-and-ai-generated-synthetic-images\\\\test\\REAL\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lb9NLM-bAb8E"
      },
      "outputs": [],
      "source": [
        "# Load image data from the directories\n",
        "train_fake_data = load_images_from_directory(train_fake_dir)\n",
        "train_real_data = load_images_from_directory(train_real_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0zMx-KGFQWAl"
      },
      "source": [
        "<h4>input & output example </h4>\n",
        "\n",
        "on the left we can see the origin image and on the right the gradientd we extracted from the image.\n",
        "\n",
        "the next step is to reduce the dimension becuase every image has 324 gradients."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "baxCGR7bAb8F",
        "outputId": "23c7896a-89da-4e02-f3b1-0f1de4cdfece"
      },
      "outputs": [],
      "source": [
        "# Compute HOG features for the datasets\n",
        "train_fake_hog = compute_hog_features(train_fake_data)\n",
        "train_real_hog = compute_hog_features(train_real_data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AsiO6U-zQWAm"
      },
      "source": [
        "<h4>Dimension reduction</h4>\n",
        "\n",
        "Lets reduce the dimension.\n",
        "and then we will see if there are some differences.\n",
        "\n",
        "first we will see it on 2D and then on 3D."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "igBrQHVJQWAm"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "num_samples = 800\n",
        "\n",
        "slice = 160\n",
        "\n",
        "# Initialize arrays to store (x, y) points\n",
        "points_fake = []\n",
        "points_real = []\n",
        "\n",
        "\n",
        "fake_sample_indices = random.sample(range(len(train_fake_hog)), num_samples)\n",
        "real_sample_indices = random.sample(range(len(train_real_hog)), num_samples)\n",
        "\n",
        "# Compute (x, y) points for fake images\n",
        "for i in fake_sample_indices:\n",
        "    fake_hog_data = train_fake_hog[i]\n",
        "\n",
        "    x = np.mean(fake_hog_data[:slice])  # Mean of the first half\n",
        "    y = np.mean(fake_hog_data[slice:])  # Mean of the second half\n",
        "    points_fake.append((x, y))\n",
        "\n",
        "# Compute (x, y) points for real images\n",
        "for j in real_sample_indices:\n",
        "    real_hog_data = train_real_hog[j]\n",
        "    x = np.mean(real_hog_data[:slice])  # Mean of the first half\n",
        "    y = np.mean(real_hog_data[slice:])  # Mean of the second half\n",
        "    points_real.append((x, y))\n",
        "\n",
        "# Convert the lists of (x, y) points to NumPy arrays\n",
        "points_fake = np.array(points_fake)\n",
        "points_real = np.array(points_real)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QpKSdB5vQWAm"
      },
      "source": [
        "<h4>2D</h4>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-q18VyzDQWAm",
        "outputId": "a5f3ddfa-d5c3-41f0-97a2-3bd9b2dc47a8"
      },
      "outputs": [],
      "source": [
        "# Create two side-by-side scatter plots for (x, y) points\n",
        "fig, axes = plt.subplots(1, 2, figsize=(8, 4))  # 1 row, 2 columns\n",
        "\n",
        "# Scatter plot for fake images\n",
        "axes[0].scatter(points_fake[:, 0], points_fake[:, 1], color='red', label='Fake Images')\n",
        "axes[0].set_xlabel('X')\n",
        "axes[0].set_ylabel('Y')\n",
        "axes[0].set_title('(X, Y) Points for Fake Images')\n",
        "\n",
        "# Scatter plot for real images\n",
        "axes[1].scatter(points_real[:, 0], points_real[:, 1], color='blue', label='Real Images')\n",
        "axes[1].set_xlabel('X')\n",
        "axes[1].set_ylabel('Y')\n",
        "axes[1].set_title('(X, Y) Points for Real Images')\n",
        "\n",
        "# Add a legend to the plots\n",
        "axes[0].legend()\n",
        "axes[1].legend()\n",
        "\n",
        "# Adjust spacing between subplots\n",
        "plt.tight_layout()\n",
        "\n",
        "# Show the plots\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mc1S1nuPQWAm"
      },
      "source": [
        "<b>2D conclusion</b>\n",
        "\n",
        "As we can see in the charts above, which contain 800 random observations per class.\n",
        "\n",
        "The main distribution is concentrated on the upper right side in both and there is no essential difference between the distribution of the gradients.\n",
        "\n",
        "We will now see if there is any difference in 3D."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KIvDiRY_QWAn"
      },
      "source": [
        "<h4>3D</h4>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aMQtNAirQWAn"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import random\n",
        "\n",
        "\n",
        "num_samples = 800\n",
        "\n",
        "slice_A = 105\n",
        "slice_B = 210\n",
        "\n",
        "# Initialize arrays to store (x, y, z) points\n",
        "points_fake = []\n",
        "points_real = []\n",
        "\n",
        "\n",
        "fake_sample_indices = random.sample(range(len(train_fake_hog)), num_samples)\n",
        "real_sample_indices = random.sample(range(len(train_real_hog)), num_samples)\n",
        "\n",
        "# Compute (x, y, z) points for fake images\n",
        "for i in fake_sample_indices:\n",
        "    fake_hog_data = train_fake_hog[i]\n",
        "    x = np.mean(fake_hog_data[:slice_A])\n",
        "    y = np.mean(fake_hog_data[slice_A:slice_B])\n",
        "    z = np.mean(fake_hog_data[slice_B:])\n",
        "    points_fake.append((x, y, z))\n",
        "\n",
        "# Compute (x, y, z) points for real images\n",
        "for j in real_sample_indices:\n",
        "    real_hog_data = train_real_hog[j]\n",
        "    x = np.mean(real_hog_data[:slice_A])\n",
        "    y = np.mean(real_hog_data[slice_A:slice_B])\n",
        "    z = np.mean(real_hog_data[slice_B:])\n",
        "    points_real.append((x, y, z))\n",
        "\n",
        "# Convert the lists of (x, y, z) points to NumPy arrays\n",
        "points_fake = np.array(points_fake)\n",
        "points_real = np.array(points_real)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jVi1SHMyQWAn",
        "outputId": "cdf2ed38-1353-4a68-f786-705f9fc9c99d"
      },
      "outputs": [],
      "source": [
        "# Create two side-by-side 3D scatter plots for (x, y, z) points\n",
        "fig = plt.figure(figsize=(8, 4))\n",
        "\n",
        "# 3D scatter plot for fake images\n",
        "ax_fake = fig.add_subplot(121, projection='3d')\n",
        "ax_fake.scatter(points_fake[:, 0], points_fake[:, 1], points_fake[:, 2], color='red', label='Fake Images')\n",
        "ax_fake.set_xlabel('X')\n",
        "ax_fake.set_ylabel('Y')\n",
        "ax_fake.set_zlabel('Z')\n",
        "ax_fake.set_title('(X, Y, Z) Points for Fake Images')\n",
        "\n",
        "# 3D scatter plot for real images\n",
        "ax_real = fig.add_subplot(122, projection='3d')\n",
        "ax_real.scatter(points_real[:, 0], points_real[:, 1], points_real[:, 2], color='blue', label='Real Images')\n",
        "ax_real.set_xlabel('X')\n",
        "ax_real.set_ylabel('Y')\n",
        "ax_real.set_zlabel('Z')\n",
        "ax_real.set_title('(X, Y, Z) Points for Real Images')\n",
        "\n",
        "# Add a legend to the plots\n",
        "ax_fake.legend()\n",
        "ax_real.legend()\n",
        "\n",
        "# Adjust spacing between subplots\n",
        "plt.tight_layout()\n",
        "\n",
        "# Show the plots\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkyMcs79QWAn"
      },
      "source": [
        "<b>3D conculsion</b>\n",
        "\n",
        "Here too we examined 800 random observations,\n",
        "\n",
        " and it can be seen that there is no significant difference in their distribution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Enh-jmbKQWAn"
      },
      "source": [
        "<b>HOGs conclution</b>\n",
        "\n",
        "After testing this algorithm, we downsized and tried both in 2D and 3D, it seems that the behavior of the images from both classes is relatively the same and it is not possible to conclude at this stage about a significant difference that will help us analyze the data.\n",
        "\n",
        "We will continue to examine different algorithms."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "________________________________________________________________________________________________________________________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1Vq0yI1QWAt"
      },
      "source": [
        "After we have seen several convolutional networks and studied the data, we can now build a neural network that we will feed it with the right variables and an ideal augmentation, from brightness level, zoom to an estimate of how many layers we will use.\n",
        "\n",
        "We hope to get good results when the level of accuracy is maximum and the loss is minimal."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ItGdNL5atJ6Q"
      },
      "source": [
        "### starts build the CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qcFb30eFk43u",
        "outputId": "0b3be6a2-a202-4bb2-bca8-531a53ad2ce3"
      },
      "outputs": [],
      "source": [
        "train_datagen = ImageDataGenerator(\n",
        "    rescale = 1./255,\n",
        "    horizontal_flip = True\n",
        ")\n",
        "training_set = train_datagen.flow_from_directory(\n",
        "    'cifake-real-and-ai-generated-synthetic-images\\\\train',\n",
        "    target_size = (64, 64),\n",
        "    batch_size = 32,\n",
        "    class_mode = 'binary'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MVc9WzU9lCBw",
        "outputId": "68fde12c-aea3-4420-8d85-c6662191e0ec"
      },
      "outputs": [],
      "source": [
        "val_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "val_set = val_datagen.flow_from_directory(\n",
        "    'cifake-real-and-ai-generated-synthetic-images\\\\val',\n",
        "    target_size = (64, 64),\n",
        "    batch_size = 32,\n",
        "    class_mode = 'binary'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yw5VCYJilFgN"
      },
      "outputs": [],
      "source": [
        "cnn = tf.keras.models.Sequential()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kP3UTMvilJHz"
      },
      "outputs": [],
      "source": [
        "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu', input_shape=[64, 64, 3]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tGbhw1R0lMkq"
      },
      "outputs": [],
      "source": [
        "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fjeu-dSNlMeU"
      },
      "outputs": [],
      "source": [
        "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu'))\n",
        "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZvIz1vrolSTO"
      },
      "outputs": [],
      "source": [
        "cnn.add(tf.keras.layers.Flatten())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y5JSJbvWlSor"
      },
      "outputs": [],
      "source": [
        "cnn.add(tf.keras.layers.Dense(units=128, activation='relu'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0IF0-a4Blb0b"
      },
      "outputs": [],
      "source": [
        "cnn.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IYwFg5aglf4o"
      },
      "outputs": [],
      "source": [
        "cnn.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KTt26TdRli4t",
        "outputId": "a463223b-5468-4e51-bbad-7a4e1e42fa7a"
      },
      "outputs": [],
      "source": [
        "cnn_history = cnn.fit(x = training_set, validation_data = val_set, epochs = 15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ETa-8dxilorR",
        "outputId": "87df04a1-9714-4ad9-ceac-70693d47de1a"
      },
      "outputs": [],
      "source": [
        "training_set.class_indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r1eZf4AkxS6c"
      },
      "outputs": [],
      "source": [
        "cnn.save('FAKE_REAL_IMG_cnn_model_V4.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CtNUKvgJxUDO"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.models.load_model('FAKE_REAL_IMG_cnn_model_V4.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s2By1kRcxYNp",
        "outputId": "b42196d0-7e96-4de0-d8b7-7c4a759167ee"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L7nk9rBPQWAv"
      },
      "outputs": [],
      "source": [
        "# Access accuracy and loss values from the history object\n",
        "train_accuracy = cnn_history.history['accuracy']\n",
        "val_accuracy = cnn_history.history['val_accuracy']\n",
        "train_loss = cnn_history.history['loss']\n",
        "val_loss = cnn_history.history['val_loss']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1SYQ8cLnQWAv",
        "outputId": "8abc3395-b21c-42a1-d76e-c5fdaff0cb89"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "num_epochs = 15\n",
        "\n",
        "\n",
        "# Create subplots for accuracy and loss\n",
        "plt.figure(figsize=(8, 4))\n",
        "\n",
        "# Accuracy subplot\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(range(1, num_epochs + 1), train_accuracy, label='Training Accuracy')\n",
        "plt.plot(range(1, num_epochs + 1), val_accuracy, label='Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.title('Accuracy Over Epochs')\n",
        "\n",
        "# Loss subplot\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(range(1, num_epochs + 1), train_loss, label='Training Loss')\n",
        "plt.plot(range(1, num_epochs + 1), val_loss, label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.title('Loss Over Epochs')\n",
        "\n",
        "# Display the plots\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26w8pvFLy_3x"
      },
      "source": [
        "# predict 10 images in every clsass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Nt3wyPgx1r4"
      },
      "outputs": [],
      "source": [
        "# reading the XLSX file\n",
        "FAKE_TEST_folder =('cifake-real-and-ai-generated-synthetic-images\\\\test\\FAKE')\n",
        "REAL_TEST_folder =('cifake-real-and-ai-generated-synthetic-images\\\\test\\REAL')\n",
        "\n",
        "fake_test_images_path = []\n",
        "real_test_images_path = []\n",
        "\n",
        "for fakefilename in os.listdir(FAKE_TEST_folder):\n",
        "  fake_img = os.path.join(FAKE_TEST_folder,fakefilename)\n",
        "  if fake_img is not None:\n",
        "      fake_test_images_path.append(fake_img)\n",
        "\n",
        "for realfilename in os.listdir(REAL_TEST_folder):\n",
        "  real_img = os.path.join(REAL_TEST_folder,realfilename)\n",
        "  if real_img is not None:\n",
        "      real_test_images_path.append(real_img)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 684
        },
        "id": "nFJACmZoy7yS",
        "outputId": "29fa7340-6caa-4ab9-d3e0-bd665be0b2d6"
      },
      "outputs": [],
      "source": [
        "# Create a figure with 5 subplots side by side\n",
        "fig, axes = plt.subplots(1, 5, figsize=(8, 4))\n",
        "fig.suptitle(\"5 predictions on fake images\", fontsize=16)\n",
        "\n",
        "for i in range(5):\n",
        "    image = cv2.imread(fake_test_images_path[i])\n",
        "    test_image = tf.keras.utils.load_img(fake_test_images_path[i], target_size = (64, 64))\n",
        "    test_image = tf.keras.utils.img_to_array(test_image)\n",
        "    test_image = np.expand_dims(test_image, axis = 0)\n",
        "    result = model.predict(test_image)\n",
        "\n",
        "    # Check if the image was loaded successfully\n",
        "    if image is not None:\n",
        "        # Displaying the image\n",
        "        axes[i].imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
        "        axes[i].set_title(result)\n",
        "    else:\n",
        "        print(f\"Error loading image {images_path[i]}\")\n",
        "\n",
        "# Remove axis labels and ticks\n",
        "for ax in axes:\n",
        "    ax.axis('off')\n",
        "\n",
        "# Show the plot\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 684
        },
        "id": "KdU3c-y8y-To",
        "outputId": "ab229224-ebea-4ab7-c964-df0569471eec"
      },
      "outputs": [],
      "source": [
        "# Create a figure with 5 subplots side by side\n",
        "fig, axes = plt.subplots(1, 5, figsize=(8, 4))\n",
        "fig.suptitle(\"5 predictions on real images\", fontsize=16)\n",
        "\n",
        "for j in range(5):\n",
        "    image = cv2.imread(real_test_images_path[j])\n",
        "    test_image = tf.keras.utils.load_img(real_test_images_path[j], target_size = (64, 64))\n",
        "    test_image = tf.keras.utils.img_to_array(test_image)\n",
        "    test_image = np.expand_dims(test_image, axis = 0)\n",
        "    result = model.predict(test_image)\n",
        "\n",
        "    # Check if the image was loaded successfully\n",
        "    if image is not None:\n",
        "        # Displaying the image\n",
        "        # Displaying the image\n",
        "        axes[j].imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
        "        axes[j].set_title(result)\n",
        "    else:\n",
        "        print(f\"Error loading image {images_path[j]}\")\n",
        "\n",
        "# Show the plot\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MBNRnIH9QWAw"
      },
      "outputs": [],
      "source": [
        "from keras.preprocessing.image import load_img, img_to_array\n",
        "from keras.applications.inception_v3 import preprocess_input\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "# Define a function to make predictions on an image\n",
        "def predict_image(image_path):\n",
        "    test_image = tf.keras.utils.load_img(image_path, target_size = (64, 64))\n",
        "    test_image = tf.keras.utils.img_to_array(test_image)\n",
        "    test_image = np.expand_dims(test_image, axis = 0)\n",
        "    result = model.predict(test_image, verbose=0)\n",
        "    return result\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CSBupb6KQWAw",
        "outputId": "c50f1559-3d0a-4b81-bbf2-aa0e55163c58"
      },
      "outputs": [],
      "source": [
        "# Specify your test folder containing \"FAKE\" and \"REAL\" subfolders\n",
        "test_folder = \"cifake-real-and-ai-generated-synthetic-images\\\\test\"\n",
        "\n",
        "# Initialize variables to store prediction results\n",
        "correct_predictions = 0\n",
        "incorrect_predictions = 0\n",
        "correct_real_predictions = 0\n",
        "correct_fake_predictions = 0\n",
        "incorrect_real_predictions = 0\n",
        "incorrect_fake_predictions = 0\n",
        "\n",
        "# Loop through the images in the test folder\n",
        "for subfolder in os.listdir(test_folder):\n",
        "    subfolder_path = os.path.join(test_folder, subfolder)\n",
        "    if not os.path.isdir(subfolder_path):\n",
        "        continue\n",
        "\n",
        "    for image_file in os.listdir(subfolder_path):\n",
        "        image_path = os.path.join(subfolder_path, image_file)\n",
        "        prediction = predict_image(image_path)\n",
        "        predicted_class = int(prediction[0][0])\n",
        "        # Assuming that the class labels are 0 for \"REAL\" and 1 for \"FAKE\"\n",
        "\n",
        "        if(subfolder == \"FAKE\"):\n",
        "            if(predicted_class == 0):\n",
        "                correct_predictions += 1\n",
        "                correct_fake_predictions += 1\n",
        "            else:\n",
        "                incorrect_predictions += 1\n",
        "                incorrect_fake_predictions += 1\n",
        "        else:\n",
        "            if(predicted_class == 1):\n",
        "                correct_predictions += 1\n",
        "                correct_real_predictions += 1\n",
        "            else:\n",
        "                incorrect_predictions += 1\n",
        "                incorrect_real_predictions += 1\n",
        "\n",
        "print(\"correct_real_predictions: \",correct_real_predictions,\n",
        "    \"\\ncorrect_fake_predictions: \", correct_fake_predictions,\n",
        "    \"\\nincorrect_real_predictions: \", incorrect_real_predictions,\n",
        "    \"\\nincorrect_fake_predictions: \", incorrect_fake_predictions\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QzrKo9PcQWAx",
        "outputId": "4431e540-9e35-4fa3-8f96-0a0025d51421"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Create histograms\n",
        "prediction_labels = [\"Correct\", \"Incorrect\"]\n",
        "prediction_counts = [correct_predictions, incorrect_predictions]\n",
        "\n",
        "real_vs_fake_labels = [\"Real Correct\", \"Fake Correct\"]\n",
        "real_vs_fake_counts = [correct_real_predictions, correct_fake_predictions]\n",
        "\n",
        "# Plot the first histogram (correct vs. incorrect predictions)\n",
        "plt.figure(figsize=(8, 4),facecolor=\"gray\")\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.bar(prediction_labels, prediction_counts)\n",
        "plt.title(\"Correct vs. Incorrect Predictions\")\n",
        "\n",
        "# Plot the second histogram (correct predictions in real vs. fake class)\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.bar(real_vs_fake_labels, real_vs_fake_counts)\n",
        "plt.title(\"Correct Predictions in Real vs. Fake Class\")\n",
        "\n",
        "# Show the histograms\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rm6KQGfftJ6V"
      },
      "source": [
        "<h1>Initial analysis</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MxLDE1v8tJ6V"
      },
      "source": [
        "In the initial analysis of the results, it's apparent that the regular CNN model has performed well with a high prediction accuracy and the CNN for the frequency domain data has mediocre results. However, there is a slight overfitting issue in the regular CNN, particularly in its ability to classify fake images, as indicated by the graph and that the CNN for the frequency domain data aggressively classifies images as fake.\n",
        "\n",
        " To further enhance the regular CNN's accuracy and maintain these good results, we may explore parameter adjustments to mitigate the overfitting problem and optimize the model's performance.\n",
        " In the CNN for the frequency domain data we will have to adjust the parameters and see how we can overcome the overfitting problem in order to get better results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNfMYP73tJ6V"
      },
      "source": [
        "<h1> Thoughts for the future</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgPrOghOtJ6W"
      },
      "source": [
        "The above concludes what I was able to accomplish for the time being: data gathering, exploratory data analysis (EDA), the incorporation of Histogram of Oriented Gradients (HOG) features, and the inception of a Convolutional Neural Network (CNN). For the submission of the full project, I hope to add:\n",
        "1. Enhance the CNN by incorporating additional parameters, such as those identified during our data analysis, to achieve superior outcomes.\n",
        "\n",
        "2. Integrate various detection methods, including Generative Adversarial Networks (GANs), to enhance precision. Employing multiple methods often yields more dependable results.\n",
        "\n",
        "3. Explore methods to simplify the images such as dimensionality reduction in order of making the learning process faster.\n",
        "\n",
        "4. Experiment with different hyperparameters, learning rates, and optimization techniques to find the best configuration for our model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "___________________________________________________________________________________________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h1>bonus: Frequency Domain</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After we saw some algorithms on the spatial domain lets examine our data in frequency domain.\n",
        "\n",
        "Frequency domain is a method of analyzing an image by examining its component frequencies rather than its pixel values. It involves transforming an image from the spatial domain (pixel-based representation) into the frequency domain (representation based on sinusoidal components like sine and cosine waves). This is useful because it allows us to uncover hidden patterns, remove noise, and enhance specific image features more effectively than in the spatial domain, making it a valuable tool for tasks like image filtering, compression, and analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h4>calculate FFT of every image</h4>\n",
        "\n",
        "Here a new dataset will be created in which each image will be replaced by its representation in the frequency domain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "# Define input and output directories\n",
        "input_dir = \"cifake-real-and-ai-generated-synthetic-images\"\n",
        "output_dir = \"freq-real-fake-images\"\n",
        "\n",
        "# Create the output directory if it doesn't exist\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "categories = [\"test\", \"train\", \"val\"]\n",
        "\n",
        "# Loop through each category\n",
        "for category in categories:\n",
        "    category_dir = os.path.join(input_dir, category)\n",
        "    output_category_dir = os.path.join(output_dir, category)\n",
        "    os.makedirs(output_category_dir, exist_ok=True)\n",
        "\n",
        "    # List of subdirectories (e.g., \"train\" and \"test\")\n",
        "    subdirectories = [\"FAKE\", \"REAL\"]\n",
        "\n",
        "    # Loop through each subdirectory\n",
        "    for subdirectory in subdirectories:\n",
        "        view_flag = 1\n",
        "        input_subdir = os.path.join(category_dir, subdirectory)\n",
        "        output_subdir = os.path.join(output_category_dir, subdirectory)\n",
        "        os.makedirs(output_subdir, exist_ok=True)\n",
        "\n",
        "        # Process each image in the subdirectory\n",
        "        for filename in os.listdir(input_subdir):\n",
        "            input_path = os.path.join(input_subdir, filename)\n",
        "            output_path = os.path.join(output_subdir, filename)\n",
        "\n",
        "            # Read the image\n",
        "            image = cv2.imread(input_path, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "            # Perform FFT (Fast Fourier Transform) to calculate the frequency domain\n",
        "            freq_domain = np.fft.fft2(image)\n",
        "            freq_domain = np.fft.fftshift(freq_domain)\n",
        "\n",
        "            # Save the frequency domain image\n",
        "            cv2.imwrite(output_path, np.abs(freq_domain).astype(np.uint8))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import cv2\n",
        "\n",
        "# Plot information\n",
        "fig, axes = plt.subplots(2, 2, figsize=(5, 5))\n",
        "fig.suptitle(\"Frequency Domain example\", fontsize=14)\n",
        "\n",
        "fake_spatial_image = cv2.imread(r\"cifake-real-and-ai-generated-synthetic-images\\\\test\\FAKE\\\\0 (2).jpg\", cv2.IMREAD_GRAYSCALE)\n",
        "fake_frequency_image = cv2.imread(r\"freq-real-fake-images\\\\test\\FAKE\\\\0 (2).jpg\", cv2.IMREAD_GRAYSCALE)\n",
        "real_spatial_image = cv2.imread(r\"cifake-real-and-ai-generated-synthetic-images\\\\test\\REAL\\\\0000 (2).jpg\", cv2.IMREAD_GRAYSCALE)\n",
        "real_frequency_image = cv2.imread(r\"freq-real-fake-images\\\\test\\REAL\\\\0000 (2).jpg\", cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "axes[0, 0].imshow(fake_spatial_image, cmap=plt.cm.gray)\n",
        "axes[0, 1].imshow(fake_frequency_image, cmap=plt.cm.gray)\n",
        "axes[1, 0].imshow(real_spatial_image, cmap=plt.cm.gray)\n",
        "axes[1, 1].imshow(real_frequency_image, cmap=plt.cm.gray)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h4>build CNN for the frequency domain data</h4>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we have the data in frequncy domain and already seperated inside the directory \"freq-real-fake-images\",\n",
        "it's ready to pattern recognition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "training_set = train_datagen.flow_from_directory(\n",
        "    'freq-real-fake-images\\\\train',\n",
        "    target_size = (64, 64),\n",
        "    batch_size = 32,\n",
        "    class_mode = 'binary'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "val_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "val_set = val_datagen.flow_from_directory('freq-real-fake-images\\\\val',\n",
        "                                            target_size = (64, 64),\n",
        "                                            batch_size = 32,\n",
        "                                            class_mode = 'binary')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cnn = tf.keras.models.Sequential()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu', input_shape=[64, 64, 3]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu'))\n",
        "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cnn.add(tf.keras.layers.Flatten())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cnn.add(tf.keras.layers.Dense(units=128, activation='relu'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cnn.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cnn.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "history = cnn.fit(x = training_set, validation_data = val_set, epochs = 12)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "training_set.class_indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cnn.save('freq_FAKE_REAL_IMG_cnn_model_V1.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "freq_model = tf.keras.models.load_model('freq_FAKE_REAL_IMG_cnn_model_V1.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "freq_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Access accuracy and loss values from the history object\n",
        "train_accuracy = history.history['accuracy']\n",
        "val_accuracy = history.history['val_accuracy']\n",
        "train_loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "num_epochs = 12\n",
        "\n",
        "\n",
        "# Create subplots for accuracy and loss\n",
        "plt.figure(figsize=(8, 4))\n",
        "\n",
        "# Accuracy subplot\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(range(1, num_epochs + 1), train_accuracy, label='Training Accuracy')\n",
        "plt.plot(range(1, num_epochs + 1), val_accuracy, label='Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.title('Accuracy Over Epochs')\n",
        "\n",
        "# Loss subplot\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(range(1, num_epochs + 1), train_loss, label='Training Loss')\n",
        "plt.plot(range(1, num_epochs + 1), val_loss, label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.title('Loss Over Epochs')\n",
        "\n",
        "# Display the plots\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from keras.preprocessing.image import load_img, img_to_array\n",
        "from keras.applications.inception_v3 import preprocess_input\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "# Define a function to make predictions on an image\n",
        "def predict_image(image_path):\n",
        "    test_image = tf.keras.utils.load_img(image_path, target_size = (64, 64))\n",
        "    test_image = tf.keras.utils.img_to_array(test_image)\n",
        "    test_image = np.expand_dims(test_image, axis = 0)\n",
        "    result = freq_model.predict(test_image, verbose=0)\n",
        "    return result\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Specify your test folder containing \"FAKE\" and \"REAL\" subfolders\n",
        "test_folder = \"freq-real-fake-images\\\\test\"\n",
        "\n",
        "# Initialize variables to store prediction results\n",
        "correct_predictions = 0\n",
        "incorrect_predictions = 0\n",
        "correct_real_predictions = 0\n",
        "correct_fake_predictions = 0\n",
        "incorrect_real_predictions = 0\n",
        "incorrect_fake_predictions = 0\n",
        "\n",
        "# Loop through the images in the test folder\n",
        "for subfolder in os.listdir(test_folder):\n",
        "    subfolder_path = os.path.join(test_folder, subfolder)\n",
        "    if not os.path.isdir(subfolder_path):\n",
        "        continue\n",
        "\n",
        "    for image_file in os.listdir(subfolder_path):\n",
        "        image_path = os.path.join(subfolder_path, image_file)\n",
        "        prediction = predict_image(image_path)\n",
        "        predicted_class = int(prediction[0][0])\n",
        "        # Assuming that the class labels are 0 for \"REAL\" and 1 for \"FAKE\"\n",
        "\n",
        "        if(subfolder == \"FAKE\"):\n",
        "            if(predicted_class == 0):\n",
        "                correct_predictions += 1\n",
        "                correct_fake_predictions += 1\n",
        "            else:\n",
        "                incorrect_predictions += 1\n",
        "                incorrect_fake_predictions += 1\n",
        "        else:\n",
        "            if(predicted_class == 1):\n",
        "                correct_predictions += 1\n",
        "                correct_real_predictions += 1\n",
        "            else:\n",
        "                incorrect_predictions += 1\n",
        "                incorrect_real_predictions += 1\n",
        "\n",
        "print(\"correct_real_predictions: \",correct_real_predictions,\n",
        "    \"\\ncorrect_fake_predictions: \", correct_fake_predictions,\n",
        "    \"\\nincorrect_real_predictions: \", incorrect_real_predictions,\n",
        "    \"\\nincorrect_fake_predictions: \", incorrect_fake_predictions\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create histograms\n",
        "prediction_labels = [\"Correct\", \"Incorrect\"]\n",
        "prediction_counts = [correct_predictions, incorrect_predictions]\n",
        "\n",
        "real_vs_fake_labels = [\"Real Correct\", \"Fake Correct\"]\n",
        "real_vs_fake_counts = [correct_real_predictions, correct_fake_predictions]\n",
        "\n",
        "# Plot the first histogram (correct vs. incorrect predictions)\n",
        "plt.figure(figsize=(8, 4),facecolor=\"gray\")\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.bar(prediction_labels, prediction_counts)\n",
        "plt.title(\"Correct vs. Incorrect Predictions\")\n",
        "\n",
        "# Plot the second histogram (correct predictions in real vs. fake class)\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.bar(real_vs_fake_labels, real_vs_fake_counts)\n",
        "plt.title(\"Correct Predictions in Real vs. Fake Class\")\n",
        "\n",
        "# Show the histograms\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
